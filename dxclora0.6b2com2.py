# -*- coding: utf-8 -*-
import json
import torch
import pandas as pd
from sqlalchemy import create_engine, Table, Column, Integer, Float, MetaData, select
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import PeftModel
from concurrent.futures import ThreadPoolExecutor
from threading import Lock

# =======================
# é…ç½®
# =======================
BASE_MODEL_PATH = "./Qwen3-0.6B"
LORA_PATHS = [
    "./lora-comment4-qwen3/final_model",
    "./lora-comment5-qwen3/final_model",
    "./lora-comment6-qwen3/final_model"
]

DB_USER = "postgres"
DB_PASSWORD = "admin"
DB_HOST = "localhost"
DB_PORT = 5432
DB_NAME = "bilibili_db"

MAX_WORKERS = 3
SAVE_INTERVAL = 50

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =======================
# æ•°æ®åº“è¿æ¥
# =======================
engine = create_engine(
    "postgresql+psycopg2://{}:{}@{}:{}/{}".format(
        DB_USER, DB_PASSWORD, DB_HOST, DB_PORT, DB_NAME
    ),
    pool_pre_ping=True
)
metadata = MetaData()
db_lock = Lock()

# =======================
# åŠ è½½æ¨¡å‹
# =======================
def load_model(thread_id, lora_path):
    print("ğŸ§± [{}] å¼€å§‹åŠ è½½æ¨¡å‹ ({})...".format(thread_id, lora_path))
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    base_model = AutoModelForSequenceClassification.from_pretrained(
        BASE_MODEL_PATH,
        num_labels=3,
        dtype=torch.float16,
        trust_remote_code=True
    ).to(device)
    model = PeftModel.from_pretrained(base_model, lora_path)
    model.to(device)
    model.eval()
    print("âœ… [{}] æ¨¡å‹åŠ è½½å®Œæˆ ({})".format(thread_id, lora_path))
    return tokenizer, model

# =======================
# çº¿ç¨‹æ‰§è¡Œå‡½æ•°
# =======================
def worker_thread(thread_id, tokenizer, model, data_records):
    table_name = "com_sentiments_lora0_6b_{}".format(thread_id + 1)
    print("ğŸš€ [{}] ä½¿ç”¨è¡¨ï¼š{}".format(thread_id, table_name))

    with db_lock:
        sentiments_table = Table(
            table_name,
            metadata,
            Column("id", Integer, primary_key=True, autoincrement=True),
            Column("com_id", Integer, nullable=False, unique=True),
            Column("sentiment", Integer, nullable=False),
            Column("confidence", Float, nullable=False),  # âœ… æ–°å¢ç½®ä¿¡åº¦å­—æ®µ
        )
        metadata.create_all(engine)

    local_conn = engine.connect()

    existing_ids = set()
    try:
        result = local_conn.execute(select(sentiments_table.c.com_id))
        existing_ids = {r[0] for r in result}
        print("ğŸ” [{}] å·²å­˜åœ¨ {} æ¡è®°å½•ï¼Œå°†è·³è¿‡è¿™äº›".format(thread_id, len(existing_ids)))
    except Exception as e:
        print("âš ï¸ [{}] æŸ¥è¯¢å·²æœ‰è®°å½•å¤±è´¥: {}".format(thread_id, e))

    buffer = []
    skipped = 0

    for i, row in enumerate(data_records, 1):
        com_id = row["id"]
        if com_id in existing_ids:
            skipped += 1
            continue

        city = row["city"]
        coms = row["com"]
        title = row["title"]
        description = row["description"]
        tags = row["tag"]

        prompt = f"""
ä½ æ˜¯æƒ…æ„Ÿåˆ†æåŠ©æ‰‹ã€‚
è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯åˆ¤æ–­è¯„è®ºå¯¹åŸå¸‚ã€Œ{city}ã€çš„æ€åº¦ã€‚**é‡ç‚¹å…³æ³¨è¯„è®ºæœ¬èº«çš„å†…å®¹{coms}**ï¼š
è¯„è®º: {coms}
è¯„è®ºé‡å¤ä¸€éä»¥å¼ºåŒ–å…¶æƒé‡: {coms}

è§†é¢‘å…ƒæ•°æ®:
- æ ‡é¢˜: {title}
- æè¿°: {description}
- æ ‡ç­¾: {tags}

è¯·è¾“å‡ºè¯„è®ºåœ¨è§†é¢‘è¯­å¢ƒä¸‹çš„æƒ…æ„Ÿå¼ºçƒˆç¨‹åº¦ï¼š
2 è¡¨ç¤ºæ”¯æŒ/ç§¯æä¸”è¡¨è¾¾å¼ºçƒˆ
1 è¡¨ç¤ºä¸­ç«‹æˆ–è¡¨è¾¾ä¸€èˆ¬
0 è¡¨ç¤ºåå¯¹/è´Ÿé¢ä¸”è¡¨è¾¾å¼ºçƒˆ
"""

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, padding="max_length", max_length=128)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        try:
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=-1)
                pred = torch.argmax(probs, dim=-1).item()
                confidence = probs[0][pred].item()  # âœ… è®¡ç®—ç½®ä¿¡åº¦
        except Exception as e:
            print("âš ï¸ [{}] com_id={} æ¨ç†å‡ºé”™: {}".format(thread_id, com_id, e))
            pred = 1
            confidence = 0.0

        buffer.append({"com_id": com_id, "sentiment": pred, "confidence": confidence})

        if i % 10 == 0:
            print("ğŸ§© [{}] å·²å¤„ç† {}/{} (è·³è¿‡ {})".format(thread_id, i, len(data_records), skipped))

        if len(buffer) >= SAVE_INTERVAL:
            try:
                local_conn.execute(sentiments_table.insert(), buffer)
                local_conn.commit()
                print("ğŸ“ [{}] å·²ä¿å­˜ {} æ¡ç»“æœ".format(thread_id, len(buffer)))
                buffer.clear()
            except Exception as e:
                print("âŒ [{}] ä¿å­˜å¤±è´¥: {}".format(thread_id, e))
                buffer.clear()

    if buffer:
        try:
            local_conn.execute(sentiments_table.insert(), buffer)
            local_conn.commit()
            print("ğŸ“ [{}] æœ€åä¿å­˜ {} æ¡ç»“æœ".format(thread_id, len(buffer)))
        except Exception as e:
            print("âŒ [{}] æœ€åä¿å­˜å¤±è´¥: {}".format(thread_id, e))

    local_conn.close()
    print("âœ… [{}] å®Œæˆ (è·³è¿‡ {} æ¡)".format(thread_id, skipped))

# =======================
# ä¸»å‡½æ•°
# =======================
def main():
    print("ğŸš€ å¯åŠ¨ {} ä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹æ¨¡å‹".format(MAX_WORKERS))

    models = []
    for i in range(MAX_WORKERS):
        tokenizer, model = load_model(i, LORA_PATHS[i])
        models.append((tokenizer, model))

    print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ")

    query = """
        SELECT a.id, b.city, b.data->>'title' as title, b.data->>'description' as description, b.data->>'tag' as tag, a.comment_text as com
        FROM video_comments as a left join videos as b on a.video_id = b.id
        WHERE b.istrue = 1
        ORDER BY id
    """
    df = pd.read_sql(query, engine)
    print("ğŸ“¦ ä¸»çº¿ç¨‹è¯»å–åˆ° {} æ¡è®°å½•".format(len(df)))

    data_for_threads = df.to_dict(orient="records")

    print("âœ… å¼€å§‹å¹¶å‘æ¨ç†")
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        for i in range(MAX_WORKERS):
            executor.submit(worker_thread, i, models[i][0], models[i][1], data_for_threads.copy())

    print("ğŸ å…¨éƒ¨çº¿ç¨‹å·²å¯åŠ¨")

if __name__ == "__main__":
    main()
